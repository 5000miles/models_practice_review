{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.01 - Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('401ksubs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer** \n",
    "* 1. How many years has he/she worked.\n",
    "* 2. Highest education level.\n",
    "* 3. If she/he has any child?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Answer**  I don't think someone's race or color should be one of the predictor, if 2 person got everything same, then they should get same idea based on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Answer** incsq: Because this features is baiscally the income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "* **Answer** : incsq & agesq. I guess SMEs have done this because this can help to expand the difference between 2 incomes or ages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "* **Answer**  I think there are two errors. inc and age. The descriptions indicate they are sqaured of inc & age. But, I think it should be the just income and age without squared. And the unit for inc should be $1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor,ExtraTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer:**\n",
    "* 1) Linear Regression: Yes, the model is easy to interpret.\n",
    "* 2) KNN: No, because the model can't be interpreted.\n",
    "* 3) Decision Tree & Extra Tree: Yes, the model can be interpreted.\n",
    "* 4) Bagging: Yes, the model can be interpreted.\n",
    "* 5) Random Forest: Yes, the model can be interpreted.\n",
    "* 6) Adaboost: Yes, the model can be interpreted.\n",
    "* 7) SVR: Yes, the model can be interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop([\"e401k\", \"p401k\", \"pira\", \"inc\", \"incsq\"], axis=1)\n",
    "y = df['inc']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=3)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "# OLS\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# knn\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# decsion tree\n",
    "dc = DecisionTreeRegressor()\n",
    "dc.fit(X_train, y_train)\n",
    "\n",
    "# bagging\n",
    "bg = BaggingRegressor()\n",
    "bg.fit(X_train, y_train)\n",
    "\n",
    "# random forest\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ada boost\n",
    "adb = AdaBoostRegressor()\n",
    "adb.fit(X_train, y_train)\n",
    "\n",
    "# SVR\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "> Bootstrapping is one of the resampling methods. Basically, we resample the data we have n times and build a model on each one of the samples. Based on the result of all the models, we can estimate the parameter of the accuracu of the parameter (standard error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "> Decision tree method suffer from high variance, however, bagged decision tree has relatively smaller variance, because we build n different models based on n resampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "> Random Forest, we build a number of decision tress on bootstrapped training samples as we did in bagging, but a random sample of m predctors is chosen as split  candidates from the full set of p predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "> For example, if there is a very strong predictors in our training dataset, then we build a number of decision trees on boostrapped training samples, those bagged tress will be highly correlated. Averaging many highly correlated quantitties won't help to reduce the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>RMSE of Training</th>\n",
       "      <th>RMSE of Testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>linear regression</td>\n",
       "      <td>20.238046</td>\n",
       "      <td>20.581316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>knn</td>\n",
       "      <td>16.437830</td>\n",
       "      <td>20.191135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>2.351666</td>\n",
       "      <td>26.155874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Bagging Decision Tree</td>\n",
       "      <td>8.686642</td>\n",
       "      <td>20.957117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>7.696809</td>\n",
       "      <td>20.329514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>24.456444</td>\n",
       "      <td>24.621227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>SVR</td>\n",
       "      <td>19.900798</td>\n",
       "      <td>20.454830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Methods  RMSE of Training  RMSE of Testing\n",
       "0      linear regression         20.238046        20.581316\n",
       "1                    knn         16.437830        20.191135\n",
       "2          Decision Tree          2.351666        26.155874\n",
       "3  Bagging Decision Tree          8.686642        20.957117\n",
       "4          Random Forest          7.696809        20.329514\n",
       "5              Ada Boost         24.456444        24.621227\n",
       "6                    SVR         19.900798        20.454830"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# rmse for training\n",
    "linreg_rmse_train = mean_squared_error(y_train, linreg.predict(X_train)) ** (1/2)\n",
    "knn_rmse_train = mean_squared_error(y_train, knn.predict(X_train)) ** (1/2)\n",
    "dc_rmse_train = mean_squared_error(y_train, dc.predict(X_train)) ** (1/2)\n",
    "bg_rmse_trian = mean_squared_error(y_train, bg.predict(X_train)) ** (1/2)\n",
    "rf_rmse_train = mean_squared_error(y_train, rf.predict(X_train)) ** (1/2)\n",
    "adb_rmse_train = mean_squared_error(y_train, adb.predict(X_train)) ** (1/2)\n",
    "svr_rmse_train = mean_squared_error(y_train, svr.predict(X_train)) ** (1/2)\n",
    "\n",
    "\n",
    "# rmse for testing\n",
    "linreg_pred = linreg.predict(X_test)\n",
    "knn_pred = knn.predict(X_test)\n",
    "dc_pred = dc.predict(X_test)\n",
    "bg_pred = bg.predict(X_test)\n",
    "rf_pred = rf.predict(X_test)\n",
    "adb_pred = adb.predict(X_test)\n",
    "svr_pred = svr.predict(X_test)\n",
    "\n",
    "linreg_rmse_test = mean_squared_error(y_test, linreg_pred) ** (1/2)\n",
    "knn_rmse_test = mean_squared_error(y_test, knn_pred) ** (1/2)\n",
    "dc_rmse_test = mean_squared_error(y_test, dc_pred) ** (1/2)\n",
    "bg_rmse_test = mean_squared_error(y_test, bg_pred) ** (1/2)\n",
    "rf_rmse_test = mean_squared_error(y_test, rf_pred) ** (1/2)\n",
    "adb_rmse_test = mean_squared_error(y_test, adb_pred) ** (1/2)\n",
    "svr_rmse_test = mean_squared_error(y_test, svr_pred) ** (1/2)\n",
    "\n",
    "rmse = {'Methods':['linear regression','knn','Decision Tree','Bagging Decision Tree','Random Forest','Ada Boost','SVR'],\n",
    "        'RMSE of Training':[linreg_rmse_train, knn_rmse_train, dc_rmse_train, bg_rmse_trian, rf_rmse_train, adb_rmse_train, svr_rmse_train],\n",
    "        'RMSE of Testing': [linreg_rmse_test, knn_rmse_test, dc_rmse_test, bg_rmse_test, rf_rmse_test, adb_rmse_test, svr_rmse_test]}\n",
    "\n",
    "rmse = pd.DataFrame(rmse)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>RMSE of Training</th>\n",
       "      <th>RMSE of Testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>linear regression</td>\n",
       "      <td>20.238046</td>\n",
       "      <td>20.581316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>knn</td>\n",
       "      <td>16.437830</td>\n",
       "      <td>20.191135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>2.351666</td>\n",
       "      <td>26.155874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Bagging Decision Tree</td>\n",
       "      <td>8.686642</td>\n",
       "      <td>20.957117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>7.696809</td>\n",
       "      <td>20.329514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>24.456444</td>\n",
       "      <td>24.621227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>SVR</td>\n",
       "      <td>19.900798</td>\n",
       "      <td>20.454830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Methods  RMSE of Training  RMSE of Testing\n",
       "0      linear regression         20.238046        20.581316\n",
       "1                    knn         16.437830        20.191135\n",
       "2          Decision Tree          2.351666        26.155874\n",
       "3  Bagging Decision Tree          8.686642        20.957117\n",
       "4          Random Forest          7.696809        20.329514\n",
       "5              Ada Boost         24.456444        24.621227\n",
       "6                    SVR         19.900798        20.454830"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse.loc[rmse['RMSE of Testing'] > rmse['RMSE of Training'],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "> All models are overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>RMSE of Training</th>\n",
       "      <th>RMSE of Testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>knn</td>\n",
       "      <td>16.437830</td>\n",
       "      <td>20.191135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>7.696809</td>\n",
       "      <td>20.329514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>SVR</td>\n",
       "      <td>19.900798</td>\n",
       "      <td>20.454830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>linear regression</td>\n",
       "      <td>20.238046</td>\n",
       "      <td>20.581316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Bagging Decision Tree</td>\n",
       "      <td>8.686642</td>\n",
       "      <td>20.957117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>24.456444</td>\n",
       "      <td>24.621227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>2.351666</td>\n",
       "      <td>26.155874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Methods  RMSE of Training  RMSE of Testing\n",
       "1                    knn         16.437830        20.191135\n",
       "4          Random Forest          7.696809        20.329514\n",
       "6                    SVR         19.900798        20.454830\n",
       "0      linear regression         20.238046        20.581316\n",
       "3  Bagging Decision Tree          8.686642        20.957117\n",
       "5              Ada Boost         24.456444        24.621227\n",
       "2          Decision Tree          2.351666        26.155874"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse.sort_values('RMSE of Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "> I would choose linear regression, because first it has a relatively low RMSE for testing data and linear regression models can be easily interepreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "> 1. I would try polynomial features\n",
    "> 2. I would transform the Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "> Because we're trying to figure out why someone is eligible for a 401K, however, if someone is already participated 401K (p401k will tell you that), then he/she must be eligible for 401K, this variable does help to make a better model result, but it doesn't help us to figure out why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "- 1) Logistic regression: yes.\n",
    "- 2) KNN: yes\n",
    "- 3) Decision Tree: yes.\n",
    "- 4) Bagged Decision Tree: yes\n",
    "- 5) Random Forest: yes\n",
    "- 6) Adaboos & XGboostt: yes\n",
    "- 7) SVM: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"e401k\", \"p401k\"], axis=1)\n",
    "y = df[\"e401k\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 3)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "knn_cat = KNeighborsClassifier()\n",
    "knn_cat.fit(X_train, y_train)\n",
    "\n",
    "dc_cat = DecisionTreeClassifier()\n",
    "dc_cat.fit(X_train, y_train)\n",
    "\n",
    "bg_cat = BaggingClassifier()\n",
    "bg_cat.fit(X_train, y_train)\n",
    "\n",
    "rf_cat = RandomForestClassifier()\n",
    "rf_cat.fit(X_train, y_train)\n",
    "\n",
    "ad_cat = AdaBoostClassifier()\n",
    "ad_cat.fit(X_train, y_train)\n",
    "\n",
    "sv_cat = SVC()\n",
    "sv_cat.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "- False Positive: The true value is negative(not eligible for 401K) but we predict its positive (eligible)\n",
    "- False Negative: The true value is positive(eligible for 401K) but we predict its negative (not eligible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "- I would prefer to minimize false negative because I would let more people to get their 401K, because I'm a nice guy and I know 401K is good for people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "- In order to optimize false negative, then we need to increase the recall, TP/(TP + FN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**\n",
    "> Because F1 score will only get a high score if both recall and precision are high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>F1 Training</th>\n",
       "      <th>F1 Testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Logistic Regreesion</td>\n",
       "      <td>0.474569</td>\n",
       "      <td>0.485089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.649651</td>\n",
       "      <td>0.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Bagging</td>\n",
       "      <td>0.965826</td>\n",
       "      <td>0.476433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.531460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.573137</td>\n",
       "      <td>0.557598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.461650</td>\n",
       "      <td>0.458969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Methods  F1 Training  F1 Testing\n",
       "0  Logistic Regreesion     0.474569    0.485089\n",
       "1                  KNN     0.649651    0.499400\n",
       "2        Decision Tree     1.000000    0.490463\n",
       "3              Bagging     0.965826    0.476433\n",
       "4        Random Forest     1.000000    0.531460\n",
       "5             AdaBoost     0.573137    0.557598\n",
       "6                  SVM     0.461650    0.458969"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For training\n",
    "logreg_train = f1_score(y_train, logreg.predict(X_train))\n",
    "knn_cat_train  = f1_score(y_train, knn_cat.predict(X_train))\n",
    "dc_cat_train = f1_score(y_train, dc_cat.predict(X_train))\n",
    "bg_cat_train = f1_score(y_train, bg_cat.predict(X_train))\n",
    "rf_cat_train = f1_score(y_train, rf_cat.predict(X_train))\n",
    "ad_cat_train = f1_score(y_train, ad_cat.predict(X_train))\n",
    "sv_cat_train = f1_score(y_train, sv_cat.predict(X_train))\n",
    "\n",
    "# For testing\n",
    "logreg_f1_test = f1_score(y_test, logreg.predict(X_test))\n",
    "knn_cat_test  = f1_score(y_test, knn_cat.predict(X_test))\n",
    "dc_cat_test = f1_score(y_test, dc_cat.predict(X_test))\n",
    "bg_cat_test = f1_score(y_test, bg_cat.predict(X_test))\n",
    "rf_cat_test = f1_score(y_test, rf_cat.predict(X_test))\n",
    "ad_cat_test = f1_score(y_test, ad_cat.predict(X_test))\n",
    "sv_cat_test = f1_score(y_test, sv_cat.predict(X_test))\n",
    "\n",
    "result_cat = pd.DataFrame({\n",
    "    'Methods':['Logistic Regreesion','KNN','Decision Tree','Bagging','Random Forest','AdaBoost','SVM'],\n",
    "    'F1 Training':[logreg_train,knn_cat_train ,dc_cat_train,bg_cat_train,rf_cat_train,ad_cat_train,sv_cat_train],\n",
    "    'F1 Testing':[logreg_f1_test,knn_cat_test,dc_cat_test,bg_cat_test,rf_cat_test,ad_cat_test,sv_cat_test]\n",
    "})\n",
    "\n",
    "result_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "Yes, models list below are overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>F1 Training</th>\n",
       "      <th>F1 Testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.649651</td>\n",
       "      <td>0.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Bagging</td>\n",
       "      <td>0.965826</td>\n",
       "      <td>0.476433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.531460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.573137</td>\n",
       "      <td>0.557598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.461650</td>\n",
       "      <td>0.458969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Methods  F1 Training  F1 Testing\n",
       "1            KNN     0.649651    0.499400\n",
       "2  Decision Tree     1.000000    0.490463\n",
       "3        Bagging     0.965826    0.476433\n",
       "4  Random Forest     1.000000    0.531460\n",
       "5       AdaBoost     0.573137    0.557598\n",
       "6            SVM     0.461650    0.458969"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cat.loc[result_cat['F1 Testing'] < result_cat['F1 Training']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "> I prefer to use AdaBoost, because first, AdaBoost has the second smallest overfit problem, secondly, it has the highest F1 testing score as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer**:\n",
    "> 1) Using GridSearch to find the best hyperparameters.\n",
    "\n",
    "> 2) Using polynimial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['marr', 'male', 'age', 'fsize', 'nettfa', 'agesq'], dtype='object')\n",
      "[ 10.24595981   1.33508397  32.8927121   -3.39214151   7.99810811\n",
      " -32.61084542]\n"
     ]
    }
   ],
   "source": [
    "X_regression = df.drop([\"e401k\", \"p401k\", \"pira\", \"inc\", \"incsq\"], axis=1)\n",
    "print(X_regression.columns)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "> Since age has the largest coef (so does agesq), it's the feature that best predict one's income. One unit(year) increase in age will make the income increase 32.89."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['inc', 'marr', 'male', 'age', 'fsize', 'nettfa', 'pira', 'incsq',\n",
      "       'agesq'],\n",
      "      dtype='object')\n",
      "[0.14 0.02 0.02 0.04 0.04 0.56 0.04 0.06 0.08]\n"
     ]
    }
   ],
   "source": [
    "X_classification = df.drop([\"e401k\", \"p401k\"], axis=1)\n",
    "print(X_classification.columns)\n",
    "print(ad_cat.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "> Since nettfa has the largest importance score, it's the feature that best whether or not one is eligible for a 401k."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis on a Natural Language Processing Task\n",
    "> Authors: Caroline Schmitt, Matt Brems\n",
    "\n",
    "---\n",
    "\n",
    "Exploratory data analysis (EDA) is a crucial part of any data science project. EDA helps us discover interesting relationships in the data, detect outliers and errors, examine our own assumptions about the data, and prepare for modeling. During EDA we might discover that we need to clean our data more conscientiously, or that we have more missing data than we realized, or that there aren't many patterns in the data (indicating that modeling may be challenging.)\n",
    "\n",
    "In this lab you'll bring in a natural language dataset and perform EDA. The dataset contains Facebook statuses taken from between 2009 and 2011 as well as personality test results associated with the users whose Facebook statuses are included.\n",
    "\n",
    "This dataset uses results from the Big Five Personality Test, also referred to as the five-factor model, which measures a person's score on five dimensions of personality:\n",
    "- **O**penness\n",
    "- **C**onscientiousness\n",
    "- **E**xtroversion\n",
    "- **A**greeableness\n",
    "- **N**euroticism\n",
    "\n",
    "Notoriously, the political consulting group Cambridge Analytica claims to have predicted the personalities of Facebook users by using those users' data, with the goal of targeting them with political ads that would be particularly persuasive given their personality type. Cambridge Analytica claims to have considered 32 unique 'groups' in the following fashion:\n",
    "- For each of the five OCEAN qualities, a user is categorized as either 'yes' or 'no'.\n",
    "- This makes for 32 different potential combinations of qualities. ($2^5 = 32$).\n",
    "\n",
    "Cambridge Analytica's methodology was then, roughly, the following:\n",
    "- Gather a large amount of data from Facebook.\n",
    "- Use this data to predict an individual's Big Five personality \"grouping.\"\n",
    "- Design political advertisements that would be particularly effective to that particular \"grouping.\" (For example, are certain advertisements particularly effective toward people with specific personality traits?)\n",
    "\n",
    "In this lab you will perform EDA to examine many relationships in the data.\n",
    "\n",
    "Exploratory data analysis can be a non-linear process, and you're encouraged to explore questions that occur to you as you work through the notebook.\n",
    "\n",
    "> **Content note**: This dataset contains real Facebook statuses scraped from 2009 to 2011, and some of the statuses contain language that is not safe for work, crude, or offensive. The full dataset is available as `mypersonality.csv`, and a sanitized version containing only statuses that passed an automated profanity check is available as `mypersonality_noprofanity.csv`. Please do not hesitate to use `mypersonality_noprofanity.csv` if you would prefer to. Please note that the automated profanity check is not foolproof. If you have any concerns about working with this dataset, please get in touch with your instructional team.\n",
    "\n",
    "---\n",
    "\n",
    "### External resources\n",
    "\n",
    "These resources are not required reading but may be of use or interest.\n",
    "\n",
    "- [Python Graph Gallery](https://python-graph-gallery.com/)\n",
    "- [Wikipedia page](https://en.wikipedia.org/wiki/Big_Five_personality_traits) on the Big Five test\n",
    "- [A short (3-4 pages) academic paper](./celli-al_wcpr13.pdf) using the `MyPersonality` dataset to model personality\n",
    "\n",
    "---\n",
    "\n",
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# this setting widens how many characters pandas will display in a column:\n",
    "pd.options.display.max_colwidth = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Import data\n",
    "\n",
    "This code is provided for you. Some columns with network and score-related data are dropped.\n",
    "\n",
    "The remaining columns will be `#AUTHID`, `STATUS`, `cEXT`, `cNEU`, `cAGR`, `cCON`, `cOPN`, and `DATE`:\n",
    "\n",
    "| Variable name | Description                                                                      |\n",
    "|---------------|----------------------------------------------------------------------------------|\n",
    "| `#AUTHID`     | Author ID code, unique per user                                                  |\n",
    "| `STATUS`      | Text of a Facebook status                                                        |\n",
    "| `cEXT`        | Author extroversion category, `y` for above median and `n` for below median      |\n",
    "| `cNEU`        | Author neuroticism category, `y` for above median and `n` for below median       |\n",
    "| `cAGR`        | Author agreeableness category, `y` for above median and `n` for below median     |\n",
    "| `cCON`        | Author conscientiousness category, `y` for above median and `n` for below median |\n",
    "| `cOPN`        | Author openness category, `y` for above median and `n` for below median          |\n",
    "| `DATE`        | Time stamp of original Facebook status                                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/mypersonality.csv')\n",
    "# df = pd.read_csv('data/mypersonality_noprofanity.csv') # comment out above & uncomment this to use mypersonality_noprofanity.csv\n",
    "\n",
    "dropcols = [\n",
    "    # these are network-related columns:\n",
    "    'NETWORKSIZE', 'BETWEENNESS', 'NBETWEENNESS', 'DENSITY',\n",
    "    'BROKERAGE', 'NBROKERAGE', 'TRANSITIVITY',\n",
    "    # these are score-related columns;\n",
    "    # we will use the catgories instead:\n",
    "    'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN'\n",
    "]\n",
    "\n",
    "df.drop(columns=dropcols, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "It's often more convenient to work with integers than strings. Convert the personality columns to 0 and 1, with 0 meaning 'below the median' and 1 meaning 'above the median.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## A first look at the dataset\n",
    "\n",
    "In this section, check:\n",
    "\n",
    "- How many observations are there in the dataset?\n",
    "- How many unique `#AUTHID` codes are there?\n",
    "- How many `y` and `n` values are in each of the personality category columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## EDA on Statuses\n",
    "\n",
    "Before we even vectorize the text, we might look at the lengths and word counts in each Facebook status. Some personality types might be more long-winded than others!\n",
    "\n",
    "### Create a new column called `status_char_length` that contains the character length of each status\n",
    "\n",
    "> Note: You can do this in one line with `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new column called `status_word_count` that contains the number of words in each status\n",
    "\n",
    "> Note: You can evaluate this based off of how many strings are separated by whitespaces; you're not required to check that each set of characters set apart by whitespaces is a word in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Longest and shortest statuses\n",
    "\n",
    "Looking at individual observations can help us get a sense of what the dataset contains.\n",
    "\n",
    "### Show the five longest and five shortest statuses based off of `status_word_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Investigating distribution of post lengths\n",
    "\n",
    "We've now seen some of the shortest and longest posts in the dataset. But how common are short posts, and how common are long posts? \n",
    "\n",
    "Use visuals to show the distributions of post lengths.\n",
    "\n",
    "> Note: There are multiple different types of visualizations you could use for this, and you could investigate this by looking at `status_word_count`, `status_char_length`, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring personality categories and individual users\n",
    "\n",
    "Because we have many posts per user for most users, doing EDA on the personality columns might be misleading. If we have 2,000 Facebook statuses from one very high-conscientiousness user, a bar chart of how many `'cCON'` statuses are associated with `1` might be misleading. We'll have to be careful about labeling and titling any visualizations we make off of the dataset.\n",
    "\n",
    "This dataset has redacted original poster names, but each user is given an `#AUTHID`. How many unique users are there? Do we have the same number of posts per user, or do we have some more posts by some users than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create a new dataframe called `unique_users` that only contains the `#AUTHID` and personality category columns\n",
    "\n",
    "If you do this correctly, it should have 250 rows and 6 columns.*\n",
    "\n",
    "(Hint: You can use the pandas [drop_duplicates()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html) method to make this easier. The only column you want to consider when deciding if a user is duplicated is the `#AUTHID` column.)\n",
    "\n",
    "> Note: *If using the `noprofanity` dataset, this number may be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `unique_users`, investigate personality\n",
    "\n",
    "For this section, perform EDA on just the unique users. Create 2-3 tables or visuals to investigate.\n",
    "\n",
    "Here are some prompts to get you started:\n",
    "\n",
    "- What proportion of above-median openness users also exhibit above-median extroversion vs. below-median extroversion? What about other pairs of personality traits?\n",
    "- Do any two personality traits appear to be correlated?\n",
    "- Are about equal numbers of users above median conscientiousness and below median conscientiousness, or is there an imbalanced split? What about the other personality traits?\n",
    "- Are any users below-median across all five personality traits? How many?\n",
    "- Are any users above-median all five personality traits? How many?\n",
    "\n",
    "For each dataframe or plot you end up with, remember to provide interpretation in markdown as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots vs. Tables\n",
    "\n",
    "(Short answer.) Explain when you might present a visualization versus when you might present a table of summary statistics. You can provide your answer using sentences or bullet points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring status length and word count based on personality\n",
    "\n",
    "### Using `groupby()`, find the mean status length and status word count for posts by users in the above-median and below-median categories of each of the personality traits\n",
    "\n",
    "> Note: Using `groupby()` five separate times is the easiest way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of post length for above- and below-median personality traits\n",
    "\n",
    "Choose one of the personality category columns (i.e. `cOPN`, `cCON`, `cEXT`, `cAGR`, or `cNEU`.) Visualize the distribution of status word counts of posts for users who are both above-median and below-median on that trait.\n",
    "\n",
    "> Note: This can be done several ways -- using seaborn or matplotlib, and as overlapping histograms or as side-by-side or stacked histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## EDA on Word Counts\n",
    "\n",
    "### Vectorize the text\n",
    "\n",
    "In order to perform EDA on word count data, we'll need to count-vectorize.\n",
    "\n",
    "Create a dataframe that contains the count-vectorized text for each Facebook status in the dataset.\n",
    "\n",
    "To do this, you might follow these steps:\n",
    "- Instantiate a `CountVectorizer` object\n",
    "- Fit the count vectorizer on the Facebook statuses\n",
    "- Store the transformed data\n",
    "- Convert to a dataframe and store\n",
    "    - Don't forget that the transformed data will need to be 'densified'. The `toarray()` or `todense()` methods will allow this.\n",
    "    - Don't forget that the `get_feature_names()` method on a fitted `CountVectorizer` object will bring you back the words learned from the dataset, which you can set as the `columns` argument when creating the dataframe.\n",
    "    \n",
    "It's up to you whether or not to keep stopwords in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the 15 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the 15 frequency of the most common words as a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating `propname`\n",
    "\n",
    "The word `propname` shows up frequently in this dataset. Show 10 statuses in the dataset that contain `propname`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide a short explanation of what you believe `propname` to be:\n",
    "\n",
    "> Note: The attached PDF also contains an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Most common words based on personality category\n",
    "\n",
    "In order to do more targeted EDA, we'll need to be able to reference not only the dataframe of vectorized statuses, but also the personality columns from the original dataframe.\n",
    "\n",
    "### Create a new dataframe with the vectorized text _and_ the personality category columns\n",
    "\n",
    "> Note: One way to do this is by using [`pd.concat`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the 25 most common words for statuses from high-cAGR users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the 25 most common words for statuses from low-cAGR users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### (BONUS) Most common bigrams:\n",
    "\n",
    "This is a bonus section and not required.\n",
    "\n",
    "Find the 10 most common bigrams in the dataset.\n",
    "\n",
    "> Note: The easiest way to do this involves instantiating a new `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (BONUS) Most common trigrams:\n",
    "\n",
    "This is a bonus section and not required.\n",
    "\n",
    "Find the 10 most common trigrams in the dataset.\n",
    "\n",
    "> Note: The easiest way to do this involves instantiating a new `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choose your own adventure\n",
    "\n",
    "By now you've looked at a lot of visualizations and frequency counts.\n",
    "\n",
    "Come up with 2-3 questions about the data, and try to answer them using descriptive statistics (like counts, averages, etc.) or visualizations.\n",
    "\n",
    "Some questions you might explore:\n",
    "\n",
    "- Have numbers been redacted, or are phone numbers, house numbers, or zip codes anywhere in the dataset?\n",
    "- `PROPNAME` has been used to redact personal names. Given that this data was scraped between 2009 and 2011, investigate if any public figures or famous people show up in the dataset, or their names have been redacted as well.\n",
    "- Is count of uppercase letters vs. lowercase letters per status related to any personality category or personality score?\n",
    "- Is _average_ word count per status related to any personality category or personality metric?\n",
    "- Is punctuation use related to personality?\n",
    "\n",
    "Or, of course, come up with your own questions to investigate.\n",
    "\n",
    "The focus here is on \"explore\" -- you might not find anything of particular interest, but don't let that discourage you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
